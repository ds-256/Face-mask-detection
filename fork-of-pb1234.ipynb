{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    \n    \n        \n   # for filename in filenames:\n\n     #   print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-07-13T03:22:20.227051Z","iopub.execute_input":"2021-07-13T03:22:20.227590Z","iopub.status.idle":"2021-07-13T03:22:20.237807Z","shell.execute_reply.started":"2021-07-13T03:22:20.227538Z","shell.execute_reply":"2021-07-13T03:22:20.236692Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install imutils\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-13T03:22:20.240348Z","iopub.execute_input":"2021-07-13T03:22:20.242216Z","iopub.status.idle":"2021-07-13T03:22:26.784718Z","shell.execute_reply.started":"2021-07-13T03:22:20.242021Z","shell.execute_reply":"2021-07-13T03:22:26.783798Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: imutils in /opt/conda/lib/python3.7/site-packages (0.5.4)\n\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.1.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\n##import matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport os","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-07-13T03:22:26.786538Z","iopub.execute_input":"2021-07-13T03:22:26.786945Z","iopub.status.idle":"2021-07-13T03:22:32.971778Z","shell.execute_reply.started":"2021-07-13T03:22:26.786896Z","shell.execute_reply":"2021-07-13T03:22:32.970939Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from keras.layers import BatchNormalization as bn","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:32.975624Z","iopub.execute_input":"2021-07-13T03:22:32.976036Z","iopub.status.idle":"2021-07-13T03:22:33.035548Z","shell.execute_reply.started":"2021-07-13T03:22:32.975996Z","shell.execute_reply":"2021-07-13T03:22:33.034723Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices('GPU')\ntry:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n    print('')","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.038583Z","iopub.execute_input":"2021-07-13T03:22:33.039027Z","iopub.status.idle":"2021-07-13T03:22:33.508797Z","shell.execute_reply.started":"2021-07-13T03:22:33.038982Z","shell.execute_reply":"2021-07-13T03:22:33.507733Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"k=os.walk('../input/facemask/face-mask-detector')","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.510509Z","iopub.execute_input":"2021-07-13T03:22:33.511143Z","iopub.status.idle":"2021-07-13T03:22:33.522882Z","shell.execute_reply.started":"2021-07-13T03:22:33.511098Z","shell.execute_reply":"2021-07-13T03:22:33.521878Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"%%bash\nap = argparse.ArgumentParser()\nap.add_argument(\"-d\", \"--dataset\", required=True,\nhelp=\"path to input dataset\")\nap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\nhelp=\"path to output loss/accuracy plot\")\nap.add_argument(\"-m\", \"--model\", type=str,\ndefault=\"mask_detector.model\",\nhelp=\"path to output face mask detector model\")\nargs= vars(ap.parse_args())\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.524796Z","iopub.execute_input":"2021-07-13T03:22:33.525493Z","iopub.status.idle":"2021-07-13T03:22:33.537451Z","shell.execute_reply.started":"2021-07-13T03:22:33.525444Z","shell.execute_reply":"2021-07-13T03:22:33.536425Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'%%bash\\nap = argparse.ArgumentParser()\\nap.add_argument(\"-d\", \"--dataset\", required=True,\\nhelp=\"path to input dataset\")\\nap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\\nhelp=\"path to output loss/accuracy plot\")\\nap.add_argument(\"-m\", \"--model\", type=str,\\ndefault=\"mask_detector.model\",\\nhelp=\"path to output face mask detector model\")\\nargs= vars(ap.parse_args())'"},"metadata":{}}]},{"cell_type":"code","source":"INIT_LR = 3e-4\nEPOCHS = 30\nBS = 251","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.539284Z","iopub.execute_input":"2021-07-13T03:22:33.539629Z","iopub.status.idle":"2021-07-13T03:22:33.547998Z","shell.execute_reply.started":"2021-07-13T03:22:33.539600Z","shell.execute_reply":"2021-07-13T03:22:33.546747Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"imgpt=list(paths.list_images(('../input/outputofnoout/new-outputs-here/')))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.549632Z","iopub.execute_input":"2021-07-13T03:22:33.550309Z","iopub.status.idle":"2021-07-13T03:22:33.666255Z","shell.execute_reply.started":"2021-07-13T03:22:33.550217Z","shell.execute_reply":"2021-07-13T03:22:33.665368Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data2=[]\nlabels2=[]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.668641Z","iopub.execute_input":"2021-07-13T03:22:33.668936Z","iopub.status.idle":"2021-07-13T03:22:33.676557Z","shell.execute_reply.started":"2021-07-13T03:22:33.668907Z","shell.execute_reply":"2021-07-13T03:22:33.675698Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for i in imgpt:\n    label=\"with_mask\"\n    image=load_img(i,target_size=(128,128))\n    image=img_to_array(image)\n    image=preprocess_input(image)\n    data2.append(image)\n    labels2.append(label)\ndata2=np.array(data2,dtype=\"float32\")\nlabels2=np.array(labels2)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:33.680339Z","iopub.execute_input":"2021-07-13T03:22:33.680689Z","iopub.status.idle":"2021-07-13T03:22:36.699891Z","shell.execute_reply.started":"2021-07-13T03:22:33.680636Z","shell.execute_reply":"2021-07-13T03:22:36.699050Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#non_face=list(paths.list_images('../input/natural-images/natural_images/car/'))+list(paths.list_images('../input/natural-images/natural_images/dog/'))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:36.701502Z","iopub.execute_input":"2021-07-13T03:22:36.701889Z","iopub.status.idle":"2021-07-13T03:22:36.707400Z","shell.execute_reply.started":"2021-07-13T03:22:36.701850Z","shell.execute_reply":"2021-07-13T03:22:36.705770Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#label","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:36.709194Z","iopub.execute_input":"2021-07-13T03:22:36.709608Z","iopub.status.idle":"2021-07-13T03:22:36.717199Z","shell.execute_reply.started":"2021-07-13T03:22:36.709568Z","shell.execute_reply":"2021-07-13T03:22:36.716006Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#data3=[]\n#labels3=[]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:36.719142Z","iopub.execute_input":"2021-07-13T03:22:36.719629Z","iopub.status.idle":"2021-07-13T03:22:36.725562Z","shell.execute_reply.started":"2021-07-13T03:22:36.719588Z","shell.execute_reply":"2021-07-13T03:22:36.724324Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\"\"\"for i in non_face:\n    label=\"no_face\"\n    image=load_img(i,target_size=(128,128))\n    image=img_to_array(image)\n    image=preprocess_input(image)\n    data3.append(image)\n    labels3.append(label)\ndata3=np.array(data3,dtype=\"float32\")\nlabels3=np.array(labels3)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:36.727363Z","iopub.execute_input":"2021-07-13T03:22:36.727801Z","iopub.status.idle":"2021-07-13T03:22:36.735984Z","shell.execute_reply.started":"2021-07-13T03:22:36.727755Z","shell.execute_reply":"2021-07-13T03:22:36.734863Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'for i in non_face:\\n    label=\"no_face\"\\n    image=load_img(i,target_size=(128,128))\\n    image=img_to_array(image)\\n    image=preprocess_input(image)\\n    data3.append(image)\\n    labels3.append(label)\\ndata3=np.array(data3,dtype=\"float32\")\\nlabels3=np.array(labels3)'"},"metadata":{}}]},{"cell_type":"code","source":"print(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(('../input/facemask/face-mask-detector/dataset/without_mask/')))\ndata1 = []\nlabels1 = []\n# loop over the image paths\nfor imagePath in imagePaths:\n    \n# extract the class label from the filename\n    label = imagePath.split(os.path.sep)[-2]\n# load the input image (224x224) and preprocess it\n    image = load_img(imagePath, target_size=(128, 128))\n    image = img_to_array(image)\n    image = preprocess_input(image)\n# update the data and labels lists, respectively\n    data1.append(image)\n    labels1.append(label)\n# convert the data and labels to NumPy arrays\ndata1 = np.array(data1, dtype=\"float32\")\nlabels1 = np.array(labels1)","metadata":{"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-13T03:22:36.737802Z","iopub.execute_input":"2021-07-13T03:22:36.738333Z","iopub.status.idle":"2021-07-13T03:22:40.590193Z","shell.execute_reply.started":"2021-07-13T03:22:36.738291Z","shell.execute_reply":"2021-07-13T03:22:40.589240Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[INFO] loading images...\n","output_type":"stream"}]},{"cell_type":"code","source":"labels=np.concatenate((labels2,labels1))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.591570Z","iopub.execute_input":"2021-07-13T03:22:40.591960Z","iopub.status.idle":"2021-07-13T03:22:40.598003Z","shell.execute_reply.started":"2021-07-13T03:22:40.591921Z","shell.execute_reply":"2021-07-13T03:22:40.596590Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.600111Z","iopub.execute_input":"2021-07-13T03:22:40.600810Z","iopub.status.idle":"2021-07-13T03:22:40.608870Z","shell.execute_reply.started":"2021-07-13T03:22:40.600752Z","shell.execute_reply":"2021-07-13T03:22:40.607900Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(1255,)"},"metadata":{}}]},{"cell_type":"code","source":"data=np.concatenate((data2,data1))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.610818Z","iopub.execute_input":"2021-07-13T03:22:40.611458Z","iopub.status.idle":"2021-07-13T03:22:40.707459Z","shell.execute_reply.started":"2021-07-13T03:22:40.611413Z","shell.execute_reply":"2021-07-13T03:22:40.706634Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.709156Z","iopub.execute_input":"2021-07-13T03:22:40.709557Z","iopub.status.idle":"2021-07-13T03:22:40.716092Z","shell.execute_reply.started":"2021-07-13T03:22:40.709516Z","shell.execute_reply":"2021-07-13T03:22:40.715080Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(1255, 128, 128, 3)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.718037Z","iopub.execute_input":"2021-07-13T03:22:40.718610Z","iopub.status.idle":"2021-07-13T03:22:40.723798Z","shell.execute_reply.started":"2021-07-13T03:22:40.718427Z","shell.execute_reply":"2021-07-13T03:22:40.722793Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#labels[2924]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.725651Z","iopub.execute_input":"2021-07-13T03:22:40.726211Z","iopub.status.idle":"2021-07-13T03:22:40.732151Z","shell.execute_reply.started":"2021-07-13T03:22:40.726169Z","shell.execute_reply":"2021-07-13T03:22:40.731087Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#lab=le.fit_transform(labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.734288Z","iopub.execute_input":"2021-07-13T03:22:40.734726Z","iopub.status.idle":"2021-07-13T03:22:40.739758Z","shell.execute_reply.started":"2021-07-13T03:22:40.734665Z","shell.execute_reply":"2021-07-13T03:22:40.738740Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#labels=to_categorical(lab)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.743747Z","iopub.execute_input":"2021-07-13T03:22:40.744207Z","iopub.status.idle":"2021-07-13T03:22:40.748770Z","shell.execute_reply.started":"2021-07-13T03:22:40.744164Z","shell.execute_reply":"2021-07-13T03:22:40.747814Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"lb = LabelEncoder()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)\n# partition the data into training and testing splits using 80% of\n# the data for training and the remaining 20% for testing\n(trainX, testX, trainY, testY) = train_test_split(data, labels\n\t,test_size=0.20, stratify=labels)\n# construct the training image generator for data augmentation\naug = ImageDataGenerator(\n\trotation_range=20,\n\tzoom_range=0.15,\n\twidth_shift_range=0.2,\n\theight_shift_range=0.2,\n\tshear_range=0.15,\n\thorizontal_flip=True,\n\tfill_mode=\"nearest\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:22:40.751129Z","iopub.execute_input":"2021-07-13T03:22:40.751585Z","iopub.status.idle":"2021-07-13T03:22:40.847031Z","shell.execute_reply.started":"2021-07-13T03:22:40.751544Z","shell.execute_reply":"2021-07-13T03:22:40.846130Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"LabelBinarizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:42.046083Z","iopub.execute_input":"2021-06-25T20:17:42.04648Z","iopub.status.idle":"2021-06-25T20:17:42.055655Z","shell.execute_reply.started":"2021-06-25T20:17:42.046435Z","shell.execute_reply":"2021-06-25T20:17:42.054332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k=tf.keras.applications.ResNet152V2()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:42.057517Z","iopub.execute_input":"2021-06-25T20:17:42.058406Z","iopub.status.idle":"2021-06-25T20:17:42.065846Z","shell.execute_reply.started":"2021-06-25T20:17:42.058359Z","shell.execute_reply":"2021-06-25T20:17:42.064425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(k.layers)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:42.067625Z","iopub.execute_input":"2021-06-25T20:17:42.068294Z","iopub.status.idle":"2021-06-25T20:17:42.075546Z","shell.execute_reply.started":"2021-06-25T20:17:42.068246Z","shell.execute_reply":"2021-06-25T20:17:42.074219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fine_tune_at=486","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:42.077958Z","iopub.execute_input":"2021-06-25T20:17:42.079015Z","iopub.status.idle":"2021-06-25T20:17:42.086228Z","shell.execute_reply.started":"2021-06-25T20:17:42.078786Z","shell.execute_reply":"2021-06-25T20:17:42.085115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k.output","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:42.088415Z","iopub.execute_input":"2021-06-25T20:17:42.089172Z","iopub.status.idle":"2021-06-25T20:17:42.096747Z","shell.execute_reply.started":"2021-06-25T20:17:42.089126Z","shell.execute_reply":"2021-06-25T20:17:42.095168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" pretrained_model=tf.keras.applications.ResNet152V2(include_top=False,input_shape=(128,128,3),pooling='avg')","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:24:09.907022Z","iopub.execute_input":"2021-07-13T03:24:09.907366Z","iopub.status.idle":"2021-07-13T03:24:16.023931Z","shell.execute_reply.started":"2021-07-13T03:24:09.907336Z","shell.execute_reply":"2021-07-13T03:24:16.023071Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model=tf.keras.Sequential([\n   pretrained_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(2,activation='softmax')])\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-25T20:17:48.962087Z","iopub.execute_input":"2021-06-25T20:17:48.962518Z","iopub.status.idle":"2021-06-25T20:17:52.155949Z","shell.execute_reply.started":"2021-06-25T20:17:48.962482Z","shell.execute_reply":"2021-06-25T20:17:52.154608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model.trainable=False","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:25:30.702541Z","iopub.execute_input":"2021-07-13T03:25:30.702935Z","iopub.status.idle":"2021-07-13T03:25:30.729928Z","shell.execute_reply.started":"2021-07-13T03:25:30.702901Z","shell.execute_reply":"2021-07-13T03:25:30.729107Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"inp=tf.keras.layers.Input(shape=[128,128,3])\nx=pretrained_model(inp,training=False)\n#x=tf.keras.layers.GlobalAveragePooling2D()(x)\nx=tf.keras.layers.Dense(1024,activation='relu')(x)\nout=tf.keras.layers.Dense(2,activation='softmax')(x)\nmodel1=tf.keras.Model(inp,out)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:25:25.987849Z","iopub.execute_input":"2021-07-13T03:25:25.988198Z","iopub.status.idle":"2021-07-13T03:25:26.917850Z","shell.execute_reply.started":"2021-07-13T03:25:25.988167Z","shell.execute_reply":"2021-07-13T03:25:26.917025Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#for layer in pretrained_model.layers[:fine_tune_at]:\n #   layer.trainable=False","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:53.379852Z","iopub.execute_input":"2021-06-25T20:17:53.380192Z","iopub.status.idle":"2021-06-25T20:17:53.386662Z","shell.execute_reply.started":"2021-06-25T20:17:53.380162Z","shell.execute_reply":"2021-06-25T20:17:53.383359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:25:33.497883Z","iopub.execute_input":"2021-07-13T03:25:33.498241Z","iopub.status.idle":"2021-07-13T03:25:33.557325Z","shell.execute_reply.started":"2021-07-13T03:25:33.498209Z","shell.execute_reply":"2021-07-13T03:25:33.556278Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 128, 128, 3)]     0         \n_________________________________________________________________\nresnet152v2 (Functional)     (None, 2048)              58331648  \n_________________________________________________________________\ndense_1 (Dense)              (None, 1024)              2098176   \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 2050      \n=================================================================\nTotal params: 60,431,874\nTrainable params: 2,100,226\nNon-trainable params: 58,331,648\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#round(model.optimizer.lr.numpy(),7)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:53.455179Z","iopub.execute_input":"2021-06-25T20:17:53.455494Z","iopub.status.idle":"2021-06-25T20:17:53.461015Z","shell.execute_reply.started":"2021-06-25T20:17:53.455439Z","shell.execute_reply":"2021-06-25T20:17:53.459761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:17:53.462938Z","iopub.execute_input":"2021-06-25T20:17:53.463462Z","iopub.status.idle":"2021-06-25T20:17:53.475639Z","shell.execute_reply.started":"2021-06-25T20:17:53.463403Z","shell.execute_reply":"2021-06-25T20:17:53.474335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch,lr):\n    if (epoch<=10):\n        return lr\n    elif (epoch>10 and epoch<=20):\n        return lr * tf.math.exp(-0.1)\n    elif (epoch>20 and epoch<=30):\n        return lr * tf.math.exp(-0.1)*0.1\n    else:\n        return lr * tf.math.exp(-0.1)*0.001\n\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:25:43.827978Z","iopub.execute_input":"2021-07-13T03:25:43.828332Z","iopub.status.idle":"2021-07-13T03:25:43.835540Z","shell.execute_reply.started":"2021-07-13T03:25:43.828301Z","shell.execute_reply":"2021-07-13T03:25:43.834517Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"callback=tf.keras.callbacks.LearningRateScheduler(scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:25:44.872417Z","iopub.execute_input":"2021-07-13T03:25:44.872796Z","iopub.status.idle":"2021-07-13T03:25:44.879243Z","shell.execute_reply.started":"2021-07-13T03:25:44.872761Z","shell.execute_reply":"2021-07-13T03:25:44.878081Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"tf.config.list_physical_devices()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:25:45.943152Z","iopub.execute_input":"2021-07-13T03:25:45.943512Z","iopub.status.idle":"2021-07-13T03:25:45.949558Z","shell.execute_reply.started":"2021-07-13T03:25:45.943480Z","shell.execute_reply":"2021-07-13T03:25:45.948373Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"},"metadata":{}}]},{"cell_type":"code","source":"    print(\"[INFO] compiling model...\")\n    opt = tf.keras.optimizers.Adam(3e-4)\n    model1.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=opt,\n\tmetrics=[\"accuracy\"])\n    # train the head of the network\n    print(\"[INFO] training head...\")\n    H = model1.fit(aug.flow(trainX, trainY, batch_size=BS),\nvalidation_data=(testX, testY),callbacks=[callback],\nepochs=10)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-13T03:25:47.747709Z","iopub.execute_input":"2021-07-13T03:25:47.748070Z","iopub.status.idle":"2021-07-13T03:26:58.383668Z","shell.execute_reply.started":"2021-07-13T03:25:47.748040Z","shell.execute_reply":"2021-07-13T03:26:58.382770Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"[INFO] compiling model...\n[INFO] training head...\nEpoch 1/10\n4/4 [==============================] - 7s 2s/step - loss: 0.3505 - accuracy: 0.8416 - val_loss: 0.0940 - val_accuracy: 0.9880\nEpoch 2/10\n4/4 [==============================] - 4s 1s/step - loss: 0.0578 - accuracy: 0.9841 - val_loss: 0.0981 - val_accuracy: 0.9880\nEpoch 3/10\n4/4 [==============================] - 4s 991ms/step - loss: 0.0538 - accuracy: 0.9890 - val_loss: 0.0827 - val_accuracy: 0.9880\nEpoch 4/10\n4/4 [==============================] - 4s 1s/step - loss: 0.0469 - accuracy: 0.9851 - val_loss: 0.0790 - val_accuracy: 0.9880\nEpoch 5/10\n4/4 [==============================] - 4s 985ms/step - loss: 0.0228 - accuracy: 0.9910 - val_loss: 0.0817 - val_accuracy: 0.9880\nEpoch 6/10\n4/4 [==============================] - 5s 1s/step - loss: 0.0193 - accuracy: 0.9930 - val_loss: 0.0891 - val_accuracy: 0.9920\nEpoch 7/10\n4/4 [==============================] - 4s 977ms/step - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.0911 - val_accuracy: 0.9920\nEpoch 8/10\n4/4 [==============================] - 4s 1s/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.0836 - val_accuracy: 0.9920\nEpoch 9/10\n4/4 [==============================] - 4s 990ms/step - loss: 0.0223 - accuracy: 0.9940 - val_loss: 0.0646 - val_accuracy: 0.9880\nEpoch 10/10\n4/4 [==============================] - 4s 1s/step - loss: 0.0119 - accuracy: 0.9940 - val_loss: 0.0536 - val_accuracy: 0.9880\n","output_type":"stream"}]},{"cell_type":"code","source":"model1.optimizer.lr.numpy()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:26:58.386138Z","iopub.execute_input":"2021-07-13T03:26:58.386555Z","iopub.status.idle":"2021-07-13T03:26:58.394804Z","shell.execute_reply.started":"2021-07-13T03:26:58.386512Z","shell.execute_reply":"2021-07-13T03:26:58.393772Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"0.0003"},"metadata":{}}]},{"cell_type":"code","source":"len(model1.layers)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:26:58.397307Z","iopub.execute_input":"2021-07-13T03:26:58.397805Z","iopub.status.idle":"2021-07-13T03:26:58.404900Z","shell.execute_reply.started":"2021-07-13T03:26:58.397762Z","shell.execute_reply":"2021-07-13T03:26:58.403896Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"fine_tune_at=516\npretrained_model.trainable=True\nfor layer in pretrained_model.layers[:fine_tune_at]:\n    layer.trainable=False\nmodel1.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(3e-4),\nmetrics=[\"accuracy\"])\nmodel1.summary()\n    # train the head of the network\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:26:58.407089Z","iopub.execute_input":"2021-07-13T03:26:58.407579Z","iopub.status.idle":"2021-07-13T03:26:58.534705Z","shell.execute_reply.started":"2021-07-13T03:26:58.407537Z","shell.execute_reply":"2021-07-13T03:26:58.533844Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 128, 128, 3)]     0         \n_________________________________________________________________\nresnet152v2 (Functional)     (None, 2048)              58331648  \n_________________________________________________________________\ndense_1 (Dense)              (None, 1024)              2098176   \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 2050      \n=================================================================\nTotal params: 60,431,874\nTrainable params: 18,189,314\nNon-trainable params: 42,242,560\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model1.optimizer.get_config()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:26:58.537558Z","iopub.execute_input":"2021-07-13T03:26:58.538259Z","iopub.status.idle":"2021-07-13T03:26:58.545556Z","shell.execute_reply.started":"2021-07-13T03:26:58.538212Z","shell.execute_reply":"2021-07-13T03:26:58.544357Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'name': 'Adam',\n 'learning_rate': 0.0003,\n 'decay': 0.0,\n 'beta_1': 0.9,\n 'beta_2': 0.999,\n 'epsilon': 1e-07,\n 'amsgrad': False}"},"metadata":{}}]},{"cell_type":"code","source":"print(\"[INFO] training head...\")\ntotal_epochs=10+27\nH = model1.fit(aug.flow(trainX, trainY, batch_size=251),epochs=total_epochs,\nvalidation_data=(testX, testY),initial_epoch=H.epoch[-1],callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:26:58.547523Z","iopub.execute_input":"2021-07-13T03:26:58.548042Z","iopub.status.idle":"2021-07-13T03:29:49.921375Z","shell.execute_reply.started":"2021-07-13T03:26:58.547998Z","shell.execute_reply":"2021-07-13T03:29:49.920463Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"[INFO] training head...\nEpoch 10/37\n4/4 [==============================] - 6s 2s/step - loss: 1.9593 - accuracy: 0.7390 - val_loss: 0.5977 - val_accuracy: 0.5498\nEpoch 11/37\n4/4 [==============================] - 4s 1s/step - loss: 0.4165 - accuracy: 0.8028 - val_loss: 0.2212 - val_accuracy: 0.9761\nEpoch 12/37\n4/4 [==============================] - 4s 1s/step - loss: 0.1358 - accuracy: 0.9691 - val_loss: 0.0441 - val_accuracy: 0.9801\nEpoch 13/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0443 - accuracy: 0.9831 - val_loss: 0.0041 - val_accuracy: 0.9960\nEpoch 14/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0397 - accuracy: 0.9910 - val_loss: 0.0058 - val_accuracy: 0.9960\nEpoch 15/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.0027 - val_accuracy: 1.0000\nEpoch 16/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0026 - val_accuracy: 1.0000\nEpoch 17/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.0045 - val_accuracy: 0.9960\nEpoch 18/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0016 - accuracy: 0.9990 - val_loss: 0.0113 - val_accuracy: 0.9960\nEpoch 19/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0106 - val_accuracy: 0.9960\nEpoch 20/37\n4/4 [==============================] - 5s 1s/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0063 - val_accuracy: 0.9960\nEpoch 21/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0030 - accuracy: 0.9980 - val_loss: 0.0049 - val_accuracy: 0.9960\nEpoch 22/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 23/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 24/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0070 - accuracy: 0.9970 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 25/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 26/37\n4/4 [==============================] - 5s 1s/step - loss: 0.0018 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 27/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 28/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 29/37\n4/4 [==============================] - 4s 998ms/step - loss: 0.0050 - accuracy: 0.9980 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 30/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0094 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 31/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 32/37\n4/4 [==============================] - 5s 1s/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 33/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 34/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 35/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0039 - accuracy: 0.9980 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 36/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0033 - accuracy: 0.9970 - val_loss: 0.0047 - val_accuracy: 0.9960\nEpoch 37/37\n4/4 [==============================] - 4s 1s/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0047 - val_accuracy: 0.9960\n","output_type":"stream"}]},{"cell_type":"code","source":"model1.optimizer.get_config()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:29:49.924935Z","iopub.execute_input":"2021-07-13T03:29:49.925238Z","iopub.status.idle":"2021-07-13T03:29:49.935928Z","shell.execute_reply.started":"2021-07-13T03:29:49.925204Z","shell.execute_reply":"2021-07-13T03:29:49.934534Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'name': 'Adam',\n 'learning_rate': 2.2282048e-33,\n 'decay': 0.0,\n 'beta_1': 0.9,\n 'beta_2': 0.999,\n 'epsilon': 1e-07,\n 'amsgrad': False}"},"metadata":{}}]},{"cell_type":"code","source":"print(\"[INFO] training head...\")\ntotal_epochs=10+27+5\nH = model1.fit(aug.flow(trainX, trainY, batch_size=BS),epochs=total_epochs,\nvalidation_data=(testX, testY),initial_epoch=H.epoch[-2],callbacks=[callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(model1.optimizer.lr.numpy(),8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"[INFO] training head...\")\ntotal_epochs=10+27+5+5\nH = model1.fit(aug.flow(trainX, trainY, batch_size=BS),epochs=total_epochs,\nvalidation_data=(testX, testY),initial_epoch=H.epoch[-3],callbacks=[callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.optimizer.get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fine_tune_at=496\npretrained_model.trainable=True\nbn.trainable=False\nfor layer in pretrained_model.layers[:fine_tune_at]:\n    layer.trainable=False\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(),\nmetrics=[\"accuracy\"])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.optimizer.get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"[INFO] training head...\")\ntotal_epochs=10+20 +15\nH = model.fit(aug.flow(trainX, trainY, batch_size=BS),epochs=total_epochs,\nvalidation_data=(testX, testY),initial_epoch=H.epoch[-2],callbacks=[callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.optimizer.get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.models.save_model(model1,'./newone_with50larsandlearningratetuning',save_format='h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:21:56.462864Z","iopub.execute_input":"2021-06-25T20:21:56.463228Z","iopub.status.idle":"2021-06-25T20:21:58.313129Z","shell.execute_reply.started":"2021-06-25T20:21:56.463197Z","shell.execute_reply":"2021-06-25T20:21:58.311476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pt1= list(paths.list_images('../input/3classmodelnewevaluation/new_dataset/with_mask/')) + list(paths.list_images('../input/3classmodelnewevaluation/new_dataset/without_mask'))\ndatae=[]\nlabelse=[]\nfor i in pt1:\n    label= i.split(os.path.sep)[-2]\n    image=load_img(i,target_size=(128,128))\n    image=img_to_array(image)\n    image=preprocess_input(image)\n    datae.append(image)\n    labelse.append(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datae=np.array(datae,dtype=\"float32\")\nlabelse=np.array(labelse)\nle=LabelEncoder()\nlabelse=le.fit_transform(labelse)\nlabelse=to_categorical(labelse)\nmodel.evaluate(datae,labelse)\npredidxs=model.predict(datae,batch_size=512)\npredidxs=np.argmax(predidxs,axis=1)\nprint(classification_report(labelse.argmax(axis=1),predidxs,target_names=le.classes_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## mtcnn-tf section","metadata":{}},{"cell_type":"code","source":"!pip install mtcnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import load_model\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\n#from google.colab.patches import cv2_imshow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"det=MTCNN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=cv2.imread('../input/outputofnoout/new-outputs-here/0blue-mask.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=load_model('../input/imp-model-for/mymodel.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k=det.detect_faces(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\nfor i in range(0,np.array(k).shape[0]):\n    x1,y1,x2,y2=k[i]['box']\n    x1=max(0,x1)\n    y1=max(0,y1)\n    x2=x2+x1\n    y2=y2+y1\n    face=img[y1:y2,x1:x2]\n    face=cv2.resize(face,(128,128))\n    face=preprocess_input(face)\n    face=np.expand_dims(face,axis=0)\n    pred=model.predict(face)\n    pred=np.argmax(pred,axis=1)\n    print(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=[]\nreal_label=[]\npred_in_pred=[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"det=MTCNN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in paths.list_images('../input/3classmodelnewevaluation/new_dataset/'):\n    label= i.split(os.path.sep)[-2]\n    real_label.append(label)\n    image=cv2.imread(i)\n    img=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    final_det=det.detect_faces(img)\n    if len(final_det)==0:\n        pred_in_pred.append(\"not readable\")\n        continue\n    else:\n        \n        x1,y1,w,h=final_det[0]['box']\n        x2=x1+w\n        y2=y1+h\n        if x1<0  or y1 < 0:\n            pred_in_pred.append(\"negative label\")\n            continue\n            \n        else:\n            face=img[y1:y2,x1:x2]\n            face=cv2.resize(face,(128,128))\n            face=img_to_array(face)\n            face=preprocess_input(face)\n            face=np.expand_dims(face,axis=0)\n            smallpred=model.predict(face)\n            smallpred=np.argmax(smallpred,axis=1)\n            pred_in_pred.append(smallpred)\n        \n        \n\n        \n        \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k=list(paths.list_images('../input/3classmodelnewevaluation/new_dataset/'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in list(paths.list_images('../input/3classmodelnewevaluation/new_dataset/'))[:1254]:\n    label= i.split(os.path.sep)[-2]\n    real_label.append(label)\n    image=cv2.imread(i)\n    img=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    final_det=det.detect_faces(img)\n    if len(final_det)==0:\n        pred_in_pred.append(\"not readable\")\n        \n    else:\n        \n        x1,y1,w,h=final_det[0]['box']\n        x1=max(0,x1)\n        y1=max(0,y1)\n        x2=x1+w\n        y2=y1+h\n            \n        face=image[y1:y2,x1:x2]\n        face=cv2.resize(face,(128,128))\n        face=img_to_array(face)\n        face=preprocess_input(face)\n        face=np.expand_dims(face,axis=0)\n        smallpred=model.predict(face)\n        smallpred=np.argmax(smallpred,axis=1)\n        pred_in_pred.append(smallpred)\n        \n        \n\n        \n        \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_label=np.array(real_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le=LabelEncoder()\nreal_label=le.fit_transform(real_label)\nreal_label=to_categorical(real_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_in_pred=np.array(pred_in_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qw=np.where(pred_in_pred=='negative label')\nqw","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qw=np.array(qw)\nqw.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pt=np.where(pred_in_pred=='not readable')\npt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pt=np.array(pt)\npt.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preed=np.delete(preed,(np.where(preed=='not readable') or  np.where(preed=='negative label')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preed.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_new=np.argmax(real_label,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_new=np.delete(real_new,(np.where(pred_in_pred=='not readable') or np.where(pred_in_pred=='negative label')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_new=np.delete(real_new,np.where(pred_in_pred=='negative label'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_new.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code=[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,len(preed)):\n    code.append(preed[i][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(real_new[:1199],code,target_names=lb.classes_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code=np.array(code)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k=np.where(pre=='negative label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions on the testing set\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\n\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n# show a nicely formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs,\n\ttarget_names=lb.classes_))\n# serialize the model to disk\nprint(\"[INFO] saving mask detector model...\")\ntf.keras.models.save_model(model=model,filepath='./mymodel.h5',save_format='h5')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.pyplot import imread,imshow\nk=imread('../input/3classmodelnewevaluation/new_dataset/with_mask/115blue-mask.jpg')\nk=cv2.resize(k,(128,128))\nk=img_to_array(k)\nk=preprocess_input(k)\nk=np.expand_dims(k,axis=0)\nmodel.predict(k)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z=imshow(imread('../input/outputofnoout/new-outputs-here/115blue-mask.jpg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MASKCONFIDENCE = 0.2\nFACECONFIDENCE = 0.5\nmyPath = '../input/facemask/face-mask-detector/examples/myexamples/profilepic4.jpg'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# USAGE\n# python detect_mask_image.py --image examples/example_01.png\n\n# import the necessary packages\nimport tensorflow as tf\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nimport argparse\nimport cv2\nimport os\nimport random\n# construct the argument parser and parse the arguments\n\"\"\"\"\"\"ap = argparse.ArgumentParser()\nap.add_argument(\"-i\", \"--image\", required=True,\n\thelp=\"path to input image\")\nap.add_argument(\"-f\", \"--face\", type=str,\n\tdefault=\"face_detector\",\n\thelp=\"path to face detector model directory\")\nap.add_argument(\"-m\", \"--model\", type=str,\n\tdefault=\"mask_detector.model\",\n\thelp=\"path to trained face mask detector model\")\nap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n\thelp=\"minimum probability to filter weak detections\")\nargs = vars(ap.parse_args())\"\"\"\"\"\"\n\n# load our serialized face detector model from disk\nprint(\"[INFO] loading face detector model...\")\nprototxtPath = os.path.sep.join(['../input/facemask/face-mask-detector/face_detector', \"deploy.prototxt\"])\nweightsPath = os.path.sep.join(['../input/facemask/face-mask-detector/face_detector',\n\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n\nprint(prototxtPath,type(prototxtPath))\n\nnet = cv2.dnn.readNet(prototxtPath, weightsPath)\n\n# load the face mask detector model from disk\nprint(\"[INFO] loading face mask detector model...\")\nmodel =load_model('/kaggle/working/mask_detector_model/')\n\n# load the input image from disk, clone it, and grab the image spatial\n# dimensions\n\n######################################################################\nchoser = random.randrange(2)\nimageid = random.randrange(100)\n\nchosepath = '/maksssksksss'+str(imageid)+'.png'\n\n# if choser == 1: chosepath = 'with_mask/'+str(imageid)+'-with-mask.jpg'\n# else : chosepath = 'without_mask/'+str(imageid)+'.jpg'\n  \n\"\"\"\n##../input/facemask/face-mask-detector/dataset/\n##myPath = '../input/facemask/face-mask-detector/examples/images'+chosepath\n\"\"\"\n\nimage_path = '../input/facemask/face-mask-detector/examples/example_03.png'\nprint(image_path)\n\nimage = cv2.imread(image_path)\nprint(image.shape)\norig = image.copy()\n(h, w) = image.shape[:2]\n\n#######################################################################\n\n# construct a blob from the image\nblob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n\t(104.0, 177.0, 123.0))\n\n# pass the blob through the network and obtain the face detections\nprint(\"[INFO] computing face detections...\")\nnet.setInput(blob)\ndetections = net.forward()\n\n# loop over the detections\nfor i in range(0, detections.shape[2]):\n\t# extract the confidence (i.e., probability) associated with\n\t# the detection\n\tconfidence = detections[0, 0, i, 2]\n\n\t# filter out weak detections by ensuring the confidence is\n\t# greater than the minimum confidence\n\tif confidence > MASKCONFIDENCE:\n\t\t# compute the (x, y)-coordinates of the bounding box for\n\t\t# the object\n\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n\n\t\t# ensure the bounding boxes fall within the dimensions of\n\t\t# the frame\n\t\t(startX, startY) = (max(0, startX), max(0, startY))\n\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n\n\t\t# extract the face ROI, convert it from BGR to RGB channel\n\t\t# ordering, resize it to 224x224, and preprocess it\n\t\tface = image[startY:endY, startX:endX]\n\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n\t\tface = cv2.resize(face, (224, 224))\n\t\tface = img_to_array(face)\n\t\tface = preprocess_input(face)\n\t\tface = np.expand_dims(face, axis=0)\n\n\t\t# pass the face through the model to determine if the face\n\t\t# has a mask or not\n\t\t(mask, withoutMask) = model.predict(face)[0]\n\n\t\t# determine the class label and color we'll use to draw\n\t\t# the bounding box and text\n#######################################################################################\n\t\tif(mask>FACECONFIDENCE or withoutMask>FACECONFIDENCE):\n\t\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n\t\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n#######################################################################################\n\t\t# include the probability in the label\n\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n\n\t\t# display the label and bounding box rectangle on the output\n\t\t# frame\n\t\tcv2.putText(image, label, (startX, startY - 10),\n\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n\n# show the output image\ncv2.imshow(\"Output\", image)\ncv2.waitKey(0)\n\n\n\"\"\"","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## facenet-pytorch","metadata":{"execution":{"iopub.status.busy":"2021-06-14T11:28:29.908073Z","iopub.status.idle":"2021-06-14T11:28:29.908882Z"}}},{"cell_type":"code","source":"#!pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0\n#import torch\n#torch.__version__\n#torch.cuda.set_per_process_memory_fraction(0.8, device=0)\n#!pip install imutils\nfrom imutils import paths\n##import matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input\n#from tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import load_model\n\n#from mtcnn.mtcnn import MTCNN\nfrom matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\nimport os\n#from google.colab.patches import cv2_imshow\n!conda  install  -y gdown\n\n!pip install facenet-pytorch\nfrom facenet_pytorch import MTCNN\n!gdown --id 1-C2ND6AJp98ff4a_BF1caIxnvM82aXii\nimport zipfile\nwith zipfile.ZipFile('./new_data.zip', 'r') as zip_ref:\n    zip_ref.extractall('./new')\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n    print('')\nmtcnn=MTCNN(select_largest=True,device='cuda')\nreal_label=[]\npred_in_pred=[]\n#model=tf.keras.models.load_model('../input/imp-model-for/mymodel.h5')\nfor i in list(paths.list_images('./new')):\n    \n    label= i.split(os.path.sep)[-2]\n    real_label.append(label)\n    image=cv2.imread(i)\n    img=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    box,prob=mtcnn.detect(img)\n    if prob[0]==None:\n        pred_in_pred.append(\"not readable\")\n    else:\n        x1,y1,x2,y2=box[0][:]\n        x1=int(max(0,x1))\n        y1=int(max(0,y1))\n        x2=int(max(0,x2))\n        y2=int(max(0,y2))\n        face=image[x1:x2,y1:y2]\n        if face.shape[0]==0 or face.shape[1]==0:\n            pred_in_pred.append(\"not readable\")\n            continue\n        else:\n            \n            face=cv2.resize(face,(128,128))\n            face=preprocess_input(face)\n            face=np.expand_dims(face,axis=0)\n            \n            smallpred=model1.predict(face)[0]\n            pred_in_pred.append(smallpred)\nlen(pred_in_pred)\npred_in_pred=np.array(pred_in_pred)\npreed=pred_in_pred\npreed=np.delete(preed,np.where(pred_in_pred=='not readable'))\nreal_label=np.array(real_label)\nreal_new=real_label\nreal_new=np.delete(real_new,np.where(pred_in_pred=='not readable'))\nlen(real_new),len(preed)\nle=LabelEncoder()\nreal_new=le.fit_transform(real_new)\nreal_new=to_categorical(real_new)\nc1=[]\nfor i in range(len(preed)):\n    c1.append(preed[i][:])\nprint(classification_report(np.argmax(real_new,axis=1),np.argmax(c1,axis=1),target_names=le.classes_))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:29:49.938826Z","iopub.execute_input":"2021-07-13T03:29:49.939302Z","iopub.status.idle":"2021-07-13T03:34:36.315833Z","shell.execute_reply.started":"2021-07-13T03:29:49.939262Z","shell.execute_reply":"2021-07-13T03:34:36.314445Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2021.5.30  |       ha878542_0         136 KB  conda-forge\n    certifi-2021.5.30          |   py37h89c1867_0         141 KB  conda-forge\n    conda-4.10.3               |   py37h89c1867_0         3.1 MB  conda-forge\n    gdown-3.13.0               |     pyhd8ed1ab_0          12 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         3.3 MB\n\nThe following NEW packages will be INSTALLED:\n\n  gdown              conda-forge/noarch::gdown-3.13.0-pyhd8ed1ab_0\n\nThe following packages will be UPDATED:\n\n  ca-certificates                      2020.6.20-hecda079_0 --> 2021.5.30-ha878542_0\n  certifi                          2020.6.20-py37hc8dfbb8_0 --> 2021.5.30-py37h89c1867_0\n  conda                                4.8.4-py37hc8dfbb8_2 --> 4.10.3-py37h89c1867_0\n\n\n\nDownloading and Extracting Packages\ncertifi-2021.5.30    | 141 KB    | ##################################### | 100% \nconda-4.10.3         | 3.1 MB    | ##################################### | 100% \nca-certificates-2021 | 136 KB    | ##################################### | 100% \ngdown-3.13.0         | 12 KB     | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nCollecting facenet-pytorch\n  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n\u001b[K     || 1.9 MB 1.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (2.23.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (7.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (1.18.5)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (0.6.0a0+35d732a)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (1.24.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision->facenet-pytorch) (1.5.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision->facenet-pytorch) (0.18.2)\nInstalling collected packages: facenet-pytorch\nSuccessfully installed facenet-pytorch-2.5.2\n\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.1.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nDownloading...\nFrom: https://drive.google.com/uc?id=1-C2ND6AJp98ff4a_BF1caIxnvM82aXii\nTo: /kaggle/working/new_data.zip\n392MB [00:03, 114MB/s]  \n              precision    recall  f1-score   support\n\n   with_mask       0.90      0.99      0.94       613\nwithout_mask       0.99      0.90      0.94       634\n\n    accuracy                           0.94      1247\n   macro avg       0.94      0.94      0.94      1247\nweighted avg       0.94      0.94      0.94      1247\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}